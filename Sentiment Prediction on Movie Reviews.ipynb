{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zyJ25uz0kSaw"
   },
   "source": [
    "# **Natural Language Processing: Sentiment Prediction using Naive Bayes Classifier**\n",
    "\n",
    "#### Instructor : Prof. Sudeshna Sarkar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: ANGANA MONDAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Roll Number: 19IE10039"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ao1nhg9RknmF"
   },
   "source": [
    "We will use Naive Bayes classifier and LSTM based classifier and compare the models by accuracy on IMDB dataset.  This dataset consists of 50k movie reviews (25k positive, 25k negative). Dataset has been taken from: https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ElRkQElWUMjG"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import keras\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "fhHRim2AUm4z"
   },
   "outputs": [],
   "source": [
    "#Loading the IMDB dataset.\n",
    "df=pd.read_csv('IMDB Dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lK_Hn2f6VMP7"
   },
   "source": [
    "# Preprocessing\n",
    "PrePrecessing that needs to be done on lower cased corpus\n",
    "\n",
    "1. Removing html tags\n",
    "2. Removing URLS\n",
    "3. Removing non alphanumeric character\n",
    "4. Removing Stopwords\n",
    "5. Performing stemming and lemmatization\n",
    "\n",
    "We use regex from re. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "5B5lHZPsVOXv"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\angan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\angan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing \n",
    "\n",
    "#lower case\n",
    "df['review']=df['review'].str.lower()\n",
    "\n",
    "#removing HTML tags\n",
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "def remove_tags(text):\n",
    "    return TAG_RE.sub(\" \", text)\n",
    "df['review']= df['review'].apply(remove_tags)\n",
    "\n",
    "#removing URLs\n",
    "def remove_urls(text):\n",
    "    return re.sub(r'http\\S+', '', text)\n",
    "df['review']= df['review'].apply(remove_urls)\n",
    "\n",
    "#removing non-alphanumeric characters\n",
    "def remove_non_alphanum(text):\n",
    "    pattern = r'[^A-Za-z0-9]+'\n",
    "    text = re.sub(pattern, \" \", text)\n",
    "    return text\n",
    "df['review']= df['review'].apply(remove_non_alphanum)\n",
    "\n",
    "#removing stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "df['review'] = df['review'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "\n",
    "#Stemming the text\n",
    "from nltk.stem import PorterStemmer\n",
    "ps=PorterStemmer()\n",
    "df['review'] = df.apply(lambda row: word_tokenize(row['review']),axis=1)\n",
    "df['review'] = df['review'].apply(lambda text: [ps.stem(word) for word in text])\n",
    "\n",
    "#Lemmatizing the text\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lem=WordNetLemmatizer()\n",
    "df['review'] = df['review'].apply(lambda text: [lem.lemmatize(word, pos='v') for word in text])\n",
    "\n",
    "df2=df\n",
    "df2['review'] = df2['review'].apply(lambda text:' '.join([str(w) for w in text]))\n",
    "\n",
    "#Now, df2 is the preprocessed dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "DyaSkfcvYGXk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Length of Sentence:  119.6626\n",
      "Total number of reviews:  50000\n",
      "Number of positive reviews:  25000\n",
      "Number of negative reviews:  25000\n",
      "Thus, Ratio of positive to negative reviews are:  1.0\n"
     ]
    }
   ],
   "source": [
    "# We print Statistics of Data like avg length of sentence , proposition of data w.r.t class labels\n",
    "\n",
    "#Average length of sentence\n",
    "num_reviews=len(df['review'])\n",
    "total_word_count=0\n",
    "for i in range(0, len(df2)):\n",
    "    total_word_count = total_word_count + len(nltk.word_tokenize(df2.iloc[i]['review']))\n",
    "print(\"Average Length of Sentence: \", total_word_count/num_reviews)\n",
    "\n",
    "#proposition of data w.r.t class labels\n",
    "num_pos = len(df2[df2['sentiment'] == 'positive'])\n",
    "num_neg = len(df2[df2['sentiment'] == 'negative'])\n",
    "print(\"Total number of reviews: \",num_reviews)\n",
    "print(\"Number of positive reviews: \",num_pos)\n",
    "print(\"Number of negative reviews: \",num_neg)\n",
    "print(\"Thus, Ratio of positive to negative reviews are: \", num_pos/num_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_FkJ-e2pUwun"
   },
   "source": [
    "# Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "eVq-mN28U_J4"
   },
   "outputs": [],
   "source": [
    "# we get reviews column from df\n",
    "reviews = df2['review']\n",
    "\n",
    "# we get labels column from df\n",
    "labels = df2['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Ljo5NquhXTXr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The encoder classes are:  ['negative' 'positive']\n"
     ]
    }
   ],
   "source": [
    "# We use label encoder to encode labels. Convert to 0/1\n",
    "encoder = LabelEncoder()\n",
    "encoded_labels = encoder.fit_transform(labels)\n",
    "\n",
    "print('The encoder classes are: ',encoder.classes_)\n",
    "# print(enc.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "wzG-C_EVWWET"
   },
   "outputs": [],
   "source": [
    "# We split the data into train and test (80% - 20%). \n",
    "# We use stratify in train_test_split so that both train and test have similar ratio of positive and negative samples.\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(reviews, encoded_labels, test_size=0.2, random_state=0, stratify=encoded_labels)\n",
    "# train_sentences, test_sentences, train_labels, test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bz1YdsSkiWCX"
   },
   "source": [
    "Approach for building vocabulary for the naive Bayes.\n",
    "\n",
    "We take the train data to build vocab. In this case, some words from the test set may not be in vocab and hence one needs to perform smoothing so that one the probability term is not zero.\n",
    " \n",
    "Also building vocab by taking all words in the train set is memory intensive, hence we build vocab by choosing the top 2000 - 3000 frequent words in the training corpus.\n",
    "\n",
    "> $ P(x_i | w_j) = \\frac{ N_{x_i,w_j}\\, +\\, \\alpha }{ N_{w_j}\\, +\\, \\alpha*d} $\n",
    "\n",
    "\n",
    "$N_{x_i,w_j}$ : Number of times feature $x_i$ appears in samples of class $w_j$\n",
    "\n",
    "$N_{w_j}$ : Total count of features in class $w_j$\n",
    "\n",
    "$\\alpha$ : Parameter for additive smoothing. Here consider $\\alpha$ = 1\n",
    "\n",
    "$d$ : Dimentionality of the feature vector  $x = [x_1,x_2,...,x_d]$. In our case its the vocab size.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "1cllNfGmUr77"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# We use Count vectorizer to get frequency of the words\n",
    "\n",
    "#max_features parameter : If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus.\n",
    "vec = CountVectorizer(max_features = 3000)\n",
    "X = vec.fit_transform (x_train) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "qzRvPjWaWUnm"
   },
   "outputs": [],
   "source": [
    "# We use laplace smoothing for words in test set not present in vocab of train set\n",
    "\n",
    "d= len(vec.get_feature_names()) #total number of features, or word count in vocabulary\n",
    "training_vocab=vec.get_feature_names() #list of features (words)\n",
    "\n",
    "#finding prob(class is positive) and prob(class is negative)\n",
    "Probability_pos = len(y_train[y_train==1])/len(y_train)\n",
    "Probability_neg = len(y_train[y_train==0])/len(y_train)\n",
    "\n",
    "#finding total number of words in positive class, and in negative class\n",
    "list_words_pos = X.toarray()[np.where(y_train==1)].sum(axis=0)\n",
    "total_words_pos = (list_words_pos).sum() #total number of words in 'positive' class label\n",
    "list_words_neg = X.toarray()[np.where(y_train==0)].sum(axis=0)\n",
    "total_words_neg = (list_words_neg).sum() #total number of words in 'negative' class label\n",
    "\n",
    "\n",
    "#finding probability(word|positive class) and probability(word|negative class)\n",
    "#for each word in training_vocab (the list of unique and most frequent words)\n",
    "#with laplace smoothing.\n",
    "#We make the prob_pos and prob_neg dictionary where key=word, value=probability\n",
    "alpha=1\n",
    "\n",
    "Prob_of_word_given_pos= {} #initialising dictionary\n",
    "Prob_of_word_given_neg= {} #initialising dictionary\n",
    "\n",
    "#for every word, we calculate probability(word|positive) and probability(word|negative), and enter them in the dicts.\n",
    "for i in range(len(training_vocab)):\n",
    "    word = training_vocab[i]\n",
    "    word_count_in_pos = list_words_pos[i]\n",
    "    word_count_in_neg = list_words_neg[i]\n",
    "    #we use log probability to prevent underflow (because probability products may become very small)\n",
    "    Prob_of_word_given_pos[word]=np.log((word_count_in_pos + alpha)/(total_words_pos + alpha * d))\n",
    "    Prob_of_word_given_neg[word]=np.log((word_count_in_neg + alpha)/(total_words_neg + alpha * d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "iE7pxWIYW1z0"
   },
   "outputs": [],
   "source": [
    "# We build the model from scratch\n",
    "\n",
    "#the following function reads a text input and returns 1, if the review is positive, else 0.\n",
    "def NP_model(text_input):\n",
    "    P=Probability_pos\n",
    "    N=Probability_neg\n",
    "    words = word_tokenize(text_input)\n",
    "    \n",
    "    for w in words:\n",
    "        if w in Prob_of_word_given_pos.keys():\n",
    "            P=P+Prob_of_word_given_pos[w]#we are adding because we use log probability\n",
    "            N=N+Prob_of_word_given_neg[w]\n",
    "        else:\n",
    "            P=P+np.log((alpha)/(total_words_pos+alpha*d))#in this case, the word is not there in training_vocab\n",
    "            N=N+np.log((alpha)/(total_words_neg+alpha*d))\n",
    "    \n",
    "    if(P>=N):\n",
    "        return 1 #positive review predicted\n",
    "    else:\n",
    "        return 0 #negative review predicted\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "AtQSl1zvW4DD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Naive Bayes Model:  84.41 %\n"
     ]
    }
   ],
   "source": [
    "# We test the model on test set and report Accuracy\n",
    "\n",
    "test_sentences_list = x_test.tolist()\n",
    "correct = 0 #count of correct predictions\n",
    "for i in range(len(test_sentences_list)):\n",
    "    sentence=test_sentences_list[i]\n",
    "    predicted = NP_model(sentence)\n",
    "    #prediction is correct if it matches with the test label\n",
    "    if(predicted==y_test[i]):\n",
    "        correct=correct+1\n",
    "\n",
    "print('Accuracy of Naive Bayes Model: ',(correct/len(test_sentences_list))*100, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WlNql0acU7sa"
   },
   "source": [
    "# *LSTM* based Classifier\n",
    "\n",
    "We use the above train test splits on an LSTM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "SkqnvbUOXoN0"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters of the model\n",
    "vocab_size = 3000 #we assume our vocab size remains same as before\n",
    "oov_tok = '<OOK>'\n",
    "embedding_dim = 100\n",
    "max_length = 200 # we choose based on statistics, for example 150 to 200\n",
    "padding_type='post'\n",
    "trunc_type='post'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "UeycEg9nZAOF"
   },
   "outputs": [],
   "source": [
    "train_sentences = x_train\n",
    "test_sentences = x_test\n",
    "train_labels = y_train\n",
    "test_labels = y_test\n",
    "\n",
    "# we tokenize sentences\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(train_sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# we convert train dataset to sequence and pad sequences\n",
    "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
    "train_padded = pad_sequences(train_sequences, padding='post', maxlen=max_length)\n",
    "\n",
    "# we convert Test dataset to sequence and pad sequences\n",
    "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
    "test_padded = pad_sequences(test_sequences, padding='post', maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Mtw3w895ZP39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 200, 100)          300000    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 128)               84480     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 24)                3096      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 25        \n",
      "=================================================================\n",
      "Total params: 387,601\n",
      "Trainable params: 387,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model initialization\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    keras.layers.Bidirectional(keras.layers.LSTM(64)),\n",
    "    keras.layers.Dense(24, activation='relu'),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# compile model\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "skmaDJMnZTzc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1125/1125 [==============================] - 92s 79ms/step - loss: 0.3766 - accuracy: 0.8364 - val_loss: 0.3156 - val_accuracy: 0.8702\n",
      "Epoch 2/5\n",
      "1125/1125 [==============================] - 83s 73ms/step - loss: 0.2733 - accuracy: 0.8899 - val_loss: 0.3065 - val_accuracy: 0.8810\n",
      "Epoch 3/5\n",
      "1125/1125 [==============================] - 84s 74ms/step - loss: 0.2419 - accuracy: 0.9047 - val_loss: 0.2945 - val_accuracy: 0.8820\n",
      "Epoch 4/5\n",
      "1125/1125 [==============================] - 83s 74ms/step - loss: 0.2092 - accuracy: 0.9201 - val_loss: 0.3558 - val_accuracy: 0.8742\n",
      "Epoch 5/5\n",
      "1125/1125 [==============================] - 84s 74ms/step - loss: 0.1877 - accuracy: 0.9286 - val_loss: 0.3527 - val_accuracy: 0.8533\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "history = model.fit(train_padded, train_labels, \n",
    "                    epochs=num_epochs, verbose=1, \n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "TjEhWEr5Zq7M"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy of LSTM Model:  0.8508\n",
      "\n",
      "\n",
      "*************Classification Report*************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.78      0.84      5000\n",
      "           1       0.81      0.92      0.86      5000\n",
      "\n",
      "    accuracy                           0.85     10000\n",
      "   macro avg       0.86      0.85      0.85     10000\n",
      "weighted avg       0.86      0.85      0.85     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We calculate accuracy on Test data\n",
    "\n",
    "# Getting probabilities\n",
    "prediction = model.predict(test_padded)\n",
    "\n",
    "# Getting labels based on probability 1 if p>= 0.5 else 0\n",
    "prediction_probability = (prediction>=0.5).astype(int)\n",
    "\n",
    "# Accuracy : using classification_report from sklearn\n",
    "accuracy_lstm = classification_report(test_labels, prediction_probability, output_dict=True)['accuracy']\n",
    "print('\\nAccuracy of LSTM Model: ',accuracy_lstm,)\n",
    "print('\\n\\n*************Classification Report*************')\n",
    "print(classification_report(test_labels, prediction_probability))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TIICV-ySOYL0"
   },
   "source": [
    "## Getting predictions for random examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "m2RmfNL3OYL0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities generated: \n",
      " [[0.9584262 ]\n",
      " [0.11119056]\n",
      " [0.08764476]]\n",
      "\n",
      "The sentiment predictions on the sentences are respectively:\n",
      "Positive Review\n",
      "Negative Review\n",
      "Negative Review\n"
     ]
    }
   ],
   "source": [
    "# reviews on which we need to predict\n",
    "sentence = [\"The movie was very touching and heart whelming\", \n",
    "            \"I have never seen a terrible movie like this\", \n",
    "            \"the movie plot is terrible but it had good acting\"]\n",
    "\n",
    "\n",
    "#preprocessing the sentences\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "def stemming(text):\n",
    "    words = word_tokenize(text)\n",
    "    stemmed_list=[]\n",
    "    for w in words:\n",
    "        stemmed_list.append(ps.stem(w))\n",
    "    return \" \".join(stemmed_list)\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatizing(text):\n",
    "    words = word_tokenize(text)\n",
    "    lemmatize_list=[]\n",
    "    for w in words:\n",
    "        lemmatize_list.append(lemmatizer.lemmatize(w))\n",
    "    return \" \".join(lemmatize_list)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "for i in range(len(sentence)):\n",
    "    sentence[i]=sentence[i].lower();\n",
    "    TAG_RE = re.compile(r'<[^>]+>')\n",
    "    sentence[i]=TAG_RE.sub(\" \", sentence[i])\n",
    "    sentence[i]=re.sub(r'http\\S+', '', sentence[i])\n",
    "    pattern = r'[^A-Za-z0-9]+'\n",
    "    sentence[i]= re.sub(pattern, \" \", sentence[i])\n",
    "    sentence[i] = ' '.join([word for word in sentence[i].split() if word not in (stop)])\n",
    "    sentence[i] = stemming(sentence[i])\n",
    "    sentence[i] = lemmatizing(sentence[i])\n",
    "\n",
    "    \n",
    "# converting to a sequence\n",
    "sequences = tokenizer.texts_to_sequences(sentence)\n",
    "\n",
    "\n",
    "# padding the sequence\n",
    "padded = pad_sequences(sequences, padding='post', maxlen=max_length)\n",
    "\n",
    "\n",
    "# Getting probabilities\n",
    "print('Probabilities generated: \\n', model.predict(padded))\n",
    "\n",
    "\n",
    "# Getting labels based on probability 1 if p>= 0.5 else 0\n",
    "predictions_generated = (model.predict(padded)>=0.5).astype(int)\n",
    "\n",
    "# Printing the predictions made by the LSTM Model\n",
    "print('\\nThe sentiment predictions on the sentences are respectively:')\n",
    "for val in predictions_generated:\n",
    "    if val[0]==1:\n",
    "        print('Positive Review')\n",
    "    else:\n",
    "        print('Negative Review')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NLP_CS60075_A21_Assn2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
