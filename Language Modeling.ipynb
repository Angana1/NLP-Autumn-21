{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Z_wN2v1RT1F"
   },
   "source": [
    "# **Natural Language Processing: Sentence Auto-Completion using Language Modeling**\n",
    "\n",
    "#### Instructor : Prof. Sudeshna Sarkar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3a35tmEySCx7"
   },
   "source": [
    "Name: Angana Mondal\n",
    "\n",
    "Roll No.: 19IE10039\n",
    "\n",
    "Department: Instrumentation Engineering\n",
    "\n",
    "Email-ID: anganmon@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9weHMmyd8fnq"
   },
   "source": [
    "## **Reading a Raw Text Corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\angan\\.conda\\envs\\nlpasgn1\\lib\\site-packages (4.10.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\angan\\.conda\\envs\\nlpasgn1\\lib\\site-packages (from beautifulsoup4) (2.2.1)\n",
      "Requirement already satisfied: urlopen in c:\\users\\angan\\.conda\\envs\\nlpasgn1\\lib\\site-packages (1.0.0)\n",
      "Requirement already satisfied: requests in c:\\users\\angan\\.conda\\envs\\nlpasgn1\\lib\\site-packages (2.26.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\angan\\.conda\\envs\\nlpasgn1\\lib\\site-packages (from requests) (1.26.6)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\angan\\.conda\\envs\\nlpasgn1\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\angan\\.conda\\envs\\nlpasgn1\\lib\\site-packages (from requests) (2021.5.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\angan\\.conda\\envs\\nlpasgn1\\lib\\site-packages (from requests) (3.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4\n",
    "!pip install urlopen\n",
    "!pip install requests\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DmSy_LOK2aGQ"
   },
   "source": [
    "Retrieve & save raw corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters in text file (excluding spaces): 151735\n",
      "Number of characters in text file (excluding spaces): 128077\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#writing the corpus to rawCorpus.txt\n",
    "content= requests.get('https://www.gutenberg.org/files/730/730-0.txt')\n",
    "content.encoding='utf-8'\n",
    "content=content.text\n",
    "start_point=content.find('CHAPTER I')\n",
    "end_point=content.find('CHAPTER XI')\n",
    "raw_corpus=content[start_point:end_point]\n",
    "text_file = open(\"rawCorpus.txt\", \"w\",encoding=\"utf-8\")\n",
    "text_file.write(raw_corpus)\n",
    "text_file.close()\n",
    "\n",
    "\n",
    "#printing number of characters including white spaces\n",
    "file = open(\"rawCorpus.txt\", \"r\",encoding=\"utf-8\")\n",
    "data = file.read()\n",
    "total_len = len(data)\n",
    "print('Number of characters in text file (excluding spaces):', total_len)\n",
    "\n",
    "\n",
    "#printing number of characters excluding white spaces\n",
    "data = data.replace(\" \",\"\")\n",
    "len_without_spaces = len(data)\n",
    "print('Number of characters in text file (excluding spaces):', len_without_spaces)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6KZIOy0Y2hzQ"
   },
   "source": [
    "Read the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "DsdBJa_l2l7g"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function TextIOWrapper.close()>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "file = open(\"rawCorpus.txt\", \"r\",encoding=\"utf-8\")\n",
    "rawReadCorpus = file.read()\n",
    "file.close\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\angan\\.conda\\envs\\nlpasgn1\\lib\\site-packages (3.6.2)\n",
      "Requirement already satisfied: regex in c:\\users\\angan\\.conda\\envs\\nlpasgn1\\lib\\site-packages (from nltk) (2021.8.28)\n",
      "Requirement already satisfied: click in c:\\users\\angan\\.conda\\envs\\nlpasgn1\\lib\\site-packages (from nltk) (8.0.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\angan\\.conda\\envs\\nlpasgn1\\lib\\site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\angan\\.conda\\envs\\nlpasgn1\\lib\\site-packages (from nltk) (4.62.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\angan\\.conda\\envs\\nlpasgn1\\lib\\site-packages (from click->nltk) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "utKtZeHq4N98"
   },
   "source": [
    "## **Preprocessing the corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "2g7eO4Dm4jIn"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\angan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing modules\n",
    "import nltk\n",
    "nltk.download('punkt') # For tokenizers\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First five sentences of preprocessed corpus are:\n",
      " CHAPTER I. \n",
      " TREATS OF THE PLACE WHERE OLIVER TWIST WAS BORN AND OF THE\n",
      "\n",
      "CIRCUMSTANCES ATTENDING HIS BIRTH\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Among other public buildings in a certain town, which for many reasons\n",
      "\n",
      "it will be prudent to refrain from mentioning, and to which I will\n",
      "\n",
      "assign no fictitious name, there is one anciently common to most towns,\n",
      "\n",
      "great or small: to wit, a workhouse; and in this workhouse was born; on\n",
      "\n",
      "a day and date which I need not trouble myself to repeat, inasmuch as\n",
      "\n",
      "it can be of no possible consequence to the reader, in this stage of\n",
      "\n",
      "the business at all events; the item of mortality whose name is\n",
      "\n",
      "prefixed to the head of this chapter. \n",
      " For a long time after it was ushered into this world of sorrow and\n",
      "\n",
      "trouble, by the parish surgeon, it remained a matter of considerable\n",
      "\n",
      "doubt whether the child would survive to bear any name at all; in which\n",
      "\n",
      "case it is somewhat more than probable that these memoirs would never\n",
      "\n",
      "have appeared; or, if they had, that being comprised within a couple of\n",
      "\n",
      "pages, they would have possessed the inestimable merit of being the\n",
      "\n",
      "most concise and faithful specimen of biography, extant in the\n",
      "\n",
      "literature of any age or country. \n",
      " Although I am not disposed to maintain that the being born in a\n",
      "\n",
      "workhouse, is in itself the most fortunate and enviable circumstance\n",
      "\n",
      "that can possibly befall a human being, I do mean to say that in this\n",
      "\n",
      "particular instance, it was the best thing for Oliver Twist that could\n",
      "\n",
      "by possibility have occurred. \n",
      " The fact is, that there was considerable\n",
      "\n",
      "difficulty in inducing Oliver to take upon himself the office of\n",
      "\n",
      "respiration,—a troublesome practice, but one which custom has rendered\n",
      "\n",
      "necessary to our easy existence; and for some time he lay gasping on a\n",
      "\n",
      "little flock mattress, rather unequally poised between this world and\n",
      "\n",
      "the next: the balance being decidedly in favour of the latter. \n",
      "\n",
      "\n",
      " *********************************************************\n",
      "\n",
      "First five words/tokens of preprocessed corpus are: ['chapter', 'i', '.', 'treats', 'of']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# TOKENIZING THE CORPUS INTO WORDS\n",
    "\n",
    "import string\n",
    "list_of_words=word_tokenize(rawReadCorpus)#tokenizing corpus into words\n",
    "\n",
    "#if the word is not alphanumeric we remove it. We also remove apostrophe and first brackets\n",
    "for word in list_of_words:\n",
    "    if word.isalnum==False or word==\"'\" or word==\"(\" or word==\")\":\n",
    "        list_of_words.remove(word)\n",
    "\n",
    "# we convert all words to lower case        \n",
    "for i in range(len(list_of_words)):\n",
    "    list_of_words[i]=list_of_words[i].lower()\n",
    "\n",
    "# we edit the words like _will_ and _have_, etc\n",
    "for i in range(len(list_of_words)):\n",
    "    word=list_of_words[i]\n",
    "    if word.startswith('_') and word.endswith('_'):\n",
    "        list_of_words[i]=word[1:len(word)-1]\n",
    "\n",
    "\n",
    "# TOKENIZING THE CORPUS INTO SENTENCES\n",
    "\n",
    "list_of_sents=sent_tokenize(rawReadCorpus)\n",
    "\n",
    "\n",
    "# Print first 5 sentences of your preprocessed corpus *** Write code ***\n",
    "print(\"First five sentences of preprocessed corpus are:\\n\", list_of_sents[0],\"\\n\", list_of_sents[1],\"\\n\", list_of_sents[2],\"\\n\", list_of_sents[3],\"\\n\", list_of_sents[4],\"\\n\")\n",
    "# Print first 5 words/tokens of your preprocessed corpus *** Write code ***\n",
    "print(\"\\n *********************************************************\\n\")\n",
    "print(\"First five words/tokens of preprocessed corpus are:\", list_of_words[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eZ75_a1QL70J"
   },
   "source": [
    "**We perform the following tasks for the given corpus:**\n",
    "1. Printing the average number of tokens per sentence.\n",
    "2. Printing the length of the longest and the shortest sentence, that contains the word 'Oliver' ('Oliver' is case-insensitive).\n",
    "3. Printing the number of unique tokens in the corpus, after stopword removal using the stopwords from NLTK (case-insensitive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\angan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing modules\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of tokens per sentence is:  29.77260273972603\n",
      "Length of the longest sentence, that contains the word 'Oliver' ('Oliver' is case-insensitive) is:  126\n",
      "Length of the shortest sentence, that contains the word 'Oliver' ('Oliver' is case-insensitive) is:  3\n",
      "Number of unique words in the corpus, after removing the stopwords:  4232\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#finding average number of tokens per sentence\n",
    "sum=0;\n",
    "for sentence in list_of_sents:\n",
    "    sum=sum+len(word_tokenize(sentence))\n",
    "avg=sum/len(list_of_sents)\n",
    "print(\"Average number of tokens per sentence is: \",avg)\n",
    "\n",
    "\n",
    "# finding the length of the longest and the shortest sentence, that contains the word 'Oliver' ('Oliver' is case-insensitive).\n",
    "longest=len(word_tokenize(list_of_sents[0]))\n",
    "shortest=len(word_tokenize(list_of_sents[0]))\n",
    "for sentence in list_of_sents:\n",
    "    if (sentence.lower()).find('oliver') != -1:\n",
    "        #print(sentence)\n",
    "        curr_len=len(word_tokenize(sentence))\n",
    "        if(curr_len)>longest:\n",
    "            longest=curr_len\n",
    "        if(curr_len)<shortest:\n",
    "            shortest=curr_len\n",
    "print(\"Length of the longest sentence, that contains the word 'Oliver' ('Oliver' is case-insensitive) is: \",longest)\n",
    "print(\"Length of the shortest sentence, that contains the word 'Oliver' ('Oliver' is case-insensitive) is: \",shortest)\n",
    "\n",
    "\n",
    "# finding the number of unique tokens in the corpus, after stopword removal using the stopwords from NLTK (case-insensitive).\n",
    "unique_words=set(list_of_words)\n",
    "list_unique_words=list(unique_words)\n",
    "without_stopwords = [word for word in list_unique_words if word not in stopwords.words('english')]\n",
    "print(\"Number of unique words in the corpus, after removing the stopwords: \",len(without_stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X5RiDR7TJjKX"
   },
   "source": [
    "## **Language Modeling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJeTSt8HM95L"
   },
   "source": [
    "\n",
    "1. **We create the following language models** on the given corpus: <br>\n",
    "    i.   Unigram <br>\n",
    "    ii.  Bigram <br>\n",
    "    iii. Trigram <br>\n",
    "\n",
    "2. **We list the top 10 bigrams, trigrams**\n",
    "(Additionally we remove those items which contain only articles, prepositions, determiners eg. \"of the\", \"in a\", etc. List top-10 bigrams/trigrams in both the original and processed models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of n-grams:\n",
      "-------------------------\n",
      "--> UNIGRAMS: \n",
      "[('chapter',), ('i',), ('.',), ('treats',), ('of',)] ...\n",
      "\n",
      "--> BIGRAMS: \n",
      "[('chapter', 'i'), ('i', '.'), ('.', 'treats'), ('treats', 'of'), ('of', 'the')] ...\n",
      "\n",
      "--> TRIGRAMS: \n",
      "[('chapter', 'i', '.'), ('i', '.', 'treats'), ('.', 'treats', 'of'), ('treats', 'of', 'the'), ('of', 'the', 'place')] ...\n",
      "\n",
      "Sample of n-grams after processing:\n",
      "-------------------------\n",
      "--> UNIGRAMS: \n",
      "[('chapter',), ('treats',), ('place',), ('oliver',), ('twist',)] ...\n",
      "\n",
      "--> BIGRAMS: \n",
      "[('chapter', 'i'), ('.', 'treats'), ('treats', 'of'), ('the', 'place'), ('place', 'where')] ...\n",
      "\n",
      "--> TRIGRAMS: \n",
      "[('chapter', 'i', '.'), ('i', '.', 'treats'), ('.', 'treats', 'of'), ('treats', 'of', 'the'), ('of', 'the', 'place')] ...\n",
      "\n",
      "\n",
      "UNIGRAMS:\n",
      "\n",
      "ngram: (',',), frequency: 2543 \n",
      "ngram: ('the',), frequency: 1700 \n",
      "ngram: ('.',), frequency: 961 \n",
      "ngram: ('and',), frequency: 856 \n",
      "ngram: ('a',), frequency: 713 \n",
      "ngram: ('of',), frequency: 673 \n",
      "ngram: ('to',), frequency: 616 \n",
      "ngram: ('”',), frequency: 568 \n",
      "ngram: ('“',), frequency: 567 \n",
      "ngram: ('’',), frequency: 476 \n",
      "\n",
      "UNIGRAMS PROCESSED:\n",
      "\n",
      "ngram: ('oliver',), frequency: 304 \n",
      "ngram: ('said',), frequency: 212 \n",
      "ngram: ('mr.',), frequency: 188 \n",
      "ngram: ('bumble',), frequency: 131 \n",
      "ngram: ('gentleman',), frequency: 107 \n",
      "ngram: ('old',), frequency: 89 \n",
      "ngram: ('sowerberry',), frequency: 80 \n",
      "ngram: ('would',), frequency: 77 \n",
      "ngram: ('boy',), frequency: 75 \n",
      "ngram: ('replied',), frequency: 74 \n",
      "\n",
      "BIGRAMS:\n",
      "\n",
      "ngram: (',', 'and'), frequency: 337 \n",
      "ngram: ('.', '“'), frequency: 323 \n",
      "ngram: (',', '”'), frequency: 216 \n",
      "ngram: ('’', 's'), frequency: 213 \n",
      "ngram: (';', 'and'), frequency: 184 \n",
      "ngram: ('”', 'said'), frequency: 180 \n",
      "ngram: ('of', 'the'), frequency: 160 \n",
      "ngram: ('!', '”'), frequency: 126 \n",
      "ngram: ('in', 'the'), frequency: 125 \n",
      "ngram: ('”', '“'), frequency: 125 \n",
      "\n",
      "BIGRAMS PROCESSED:\n",
      "\n",
      "ngram: ('”', 'said'), frequency: 180 \n",
      "ngram: ('mr.', 'bumble'), frequency: 115 \n",
      "ngram: ('said', 'the'), frequency: 90 \n",
      "ngram: ('”', 'replied'), frequency: 67 \n",
      "ngram: (',', 'sir'), frequency: 59 \n",
      "ngram: ('oliver', ','), frequency: 57 \n",
      "ngram: ('the', 'old'), frequency: 53 \n",
      "ngram: (',', 'oliver'), frequency: 47 \n",
      "ngram: ('bumble', ','), frequency: 46 \n",
      "ngram: ('the', 'undertaker'), frequency: 45 \n",
      "\n",
      "TRIGRAMS:\n",
      "\n",
      "ngram: (',', '”', 'said'), frequency: 112 \n",
      "ngram: ('”', 'said', 'the'), frequency: 90 \n",
      "ngram: (',', '”', 'replied'), frequency: 60 \n",
      "ngram: ('!', '”', 'said'), frequency: 42 \n",
      "ngram: ('mr.', 'bumble', ','), frequency: 38 \n",
      "ngram: ('don', '’', 't'), frequency: 36 \n",
      "ngram: (',', 'sir', ','), frequency: 35 \n",
      "ngram: ('?', '”', '“'), frequency: 35 \n",
      "ngram: ('.', '“', 'i'), frequency: 35 \n",
      "ngram: ('”', 'said', 'mr.'), frequency: 33 \n",
      "\n",
      "TRIGRAMS PROCESSED:\n",
      "\n",
      "ngram: (',', '”', 'said'), frequency: 112 \n",
      "ngram: ('”', 'said', 'the'), frequency: 90 \n",
      "ngram: (',', '”', 'replied'), frequency: 60 \n",
      "ngram: ('!', '”', 'said'), frequency: 42 \n",
      "ngram: ('mr.', 'bumble', ','), frequency: 38 \n",
      "ngram: (',', 'sir', ','), frequency: 35 \n",
      "ngram: ('”', 'said', 'mr.'), frequency: 33 \n",
      "ngram: ('the', 'old', 'gentleman'), frequency: 33 \n",
      "ngram: ('oliver', '’', 's'), frequency: 29 \n",
      "ngram: ('sir', ',', '”'), frequency: 26 \n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Computing unprocessed ngrams\n",
    "\n",
    "unigrams=[]\n",
    "bigrams=[]\n",
    "trigrams=[]\n",
    "\n",
    "unigrams.extend(ngrams(list_of_words,1))\n",
    "bigrams.extend(ngrams(list_of_words,2))\n",
    "trigrams.extend(ngrams(list_of_words,3))\n",
    "\n",
    "print (\"Sample of n-grams:\\n\" + \"-------------------------\")\n",
    "print (\"--> UNIGRAMS: \\n\" + str(unigrams[:5]) + \" ...\\n\")\n",
    "print (\"--> BIGRAMS: \\n\" + str(bigrams[:5]) + \" ...\\n\")\n",
    "print (\"--> TRIGRAMS: \\n\" + str(trigrams[:5]) + \" ...\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Computing Processed ngrams\n",
    "\n",
    "unigrams_Processed = []\n",
    "bigrams_Processed = []\n",
    "trigrams_Processed = []\n",
    "\n",
    "punctuations=['.',',',';',':','!','”','“','?',\"'\",'’','(',')']   \n",
    "list_of_stopwords=list(stopwords.words('english'))\n",
    "to_be_removed=punctuations+list_of_stopwords\n",
    "\n",
    "for tup in unigrams:\n",
    "    if tup[0] not in to_be_removed:\n",
    "        unigrams_Processed.append(tup)\n",
    "\n",
    "for tup in bigrams:\n",
    "    if tup[0] in to_be_removed and tup[1] in to_be_removed:\n",
    "        continue\n",
    "    else:\n",
    "        bigrams_Processed.append(tup)\n",
    "        \n",
    "for tup in trigrams:\n",
    "    if tup[0] in to_be_removed and tup[1] in to_be_removed and tup[2] in to_be_removed:\n",
    "        continue\n",
    "    else:\n",
    "        trigrams_Processed.append(tup)\n",
    "\n",
    "print (\"Sample of n-grams after processing:\\n\" + \"-------------------------\")\n",
    "print (\"--> UNIGRAMS: \\n\" + str(unigrams_Processed[:5]) + \" ...\\n\")\n",
    "print (\"--> BIGRAMS: \\n\" + str(bigrams_Processed[:5]) + \" ...\\n\")\n",
    "print (\"--> TRIGRAMS: \\n\" + str(trigrams_Processed[:5]) + \" ...\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Getting frequency distribution for both processed and unprocessed ngrams\n",
    "\n",
    "def get_ngrams_freqDist(n, ngramList):\n",
    "    #This function computes the frequency corresponding to each ngram in ngramList \n",
    "    #Here, n=1 for unigram, n=2 for bigram, etc.\n",
    "    #ngramList = list of unigrams when n=1, ngramList = list of bigrams when n=2\n",
    "    #Returns: ngram_freq_dict (a Python dictionary where key = a ngram, value = its frequency)\n",
    "    \n",
    "    # *** Write code ***\n",
    "    ngram_freq_dict = {}\n",
    "\n",
    "    for tup in ngramList:\n",
    "        ngram_freq_dict[tup]=ngram_freq_dict.get(tup,0)+1\n",
    "        \n",
    "    return ngram_freq_dict\n",
    "\n",
    "unigrams_freqDist = get_ngrams_freqDist(1, unigrams)\n",
    "unigrams_Processed_freqDist = get_ngrams_freqDist(1, unigrams_Processed)\n",
    "bigrams_freqDist = get_ngrams_freqDist(2, bigrams)\n",
    "bigrams_Processed_freqDist = get_ngrams_freqDist(2, bigrams_Processed)\n",
    "trigrams_freqDist = get_ngrams_freqDist(3, trigrams)\n",
    "trigrams_Processed_freqDist = get_ngrams_freqDist(3, trigrams_Processed)                                                  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Print top 10 unigrams, having highest frequency as in unigrams_freqDist\n",
    "print(\"\\nUNIGRAMS:\\n\")\n",
    "new_dict={k: v for k, v in sorted(unigrams_freqDist.items(), key=lambda ngram: ngram[1],reverse=True)}\n",
    "for x in list(new_dict)[0:10]:\n",
    "    print (\"ngram: {}, frequency: {} \".format(x,  new_dict[x]))\n",
    "\n",
    "# Print top 10 unigrams, having highest frequency as in unigrams_Processed_freqDist\n",
    "print(\"\\nUNIGRAMS PROCESSED:\\n\")\n",
    "new_dict={k: v for k, v in sorted(unigrams_Processed_freqDist.items(), key=lambda ngram: ngram[1],reverse=True)}\n",
    "for x in list(new_dict)[0:10]:\n",
    "    print (\"ngram: {}, frequency: {} \".format(x,  new_dict[x]))\n",
    "\n",
    "# Print top 10 bigrams, having highest frequency as in bigrams_freqDist\n",
    "print(\"\\nBIGRAMS:\\n\")\n",
    "new_dict={k: v for k, v in sorted(bigrams_freqDist.items(), key=lambda ngram: ngram[1],reverse=True)}\n",
    "for x in list(new_dict)[0:10]:\n",
    "    print (\"ngram: {}, frequency: {} \".format(x,  new_dict[x]))\n",
    "\n",
    "# Print top 10 bigrams, having highest frequency as in bigrams_Processed_freqDist\n",
    "print(\"\\nBIGRAMS PROCESSED:\\n\")\n",
    "new_dict={k: v for k, v in sorted(bigrams_Processed_freqDist.items(), key=lambda ngram: ngram[1],reverse=True)}\n",
    "for x in list(new_dict)[0:10]:\n",
    "    print (\"ngram: {}, frequency: {} \".format(x,  new_dict[x]))\n",
    "\n",
    "# Print top 10 trigrams, having highest frequency as in trigrams_freqDist\n",
    "print(\"\\nTRIGRAMS:\\n\")\n",
    "new_dict={k: v for k, v in sorted(trigrams_freqDist.items(), key=lambda ngram: ngram[1],reverse=True)}\n",
    "for x in list(new_dict)[0:10]:\n",
    "    print (\"ngram: {}, frequency: {} \".format(x,  new_dict[x]))\n",
    "\n",
    "# Print top 10 trigrams, having highest frequency as in trigrams_Processed_freqDist\n",
    "print(\"\\nTRIGRAMS PROCESSED:\\n\")\n",
    "new_dict={k: v for k, v in sorted(trigrams_Processed_freqDist.items(), key=lambda ngram: ngram[1],reverse=True)}\n",
    "for x in list(new_dict)[0:10]:\n",
    "    print (\"ngram: {}, frequency: {} \".format(x,  new_dict[x]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lqu8nVV7NREo"
   },
   "source": [
    "## **Next three words' Prediction using Smoothed Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a2vnIM26b2WA"
   },
   "source": [
    "For a bigram model, add-one smoothing is defined by $P_{Add-1}(w_i|w_{i-1})=\\frac{count(w_{i-1},w_i)+1}{count(w_{i-1})+V}$.\n",
    "That is, pretend we saw each word one more time than we did.\n",
    "\n",
    "First, we compute the smoothed bigram and trigram models from the bigrams_freqDist and trigrams_freqDist we calculated above (use the unprocessed models). Second, using these smoothed models, we predict the next 3 possible word sequences for testSent1, testSent2 and testSent3, using your smoothed models.\n",
    "\n",
    "As for example, for the string 'Raj has a' the answers can be as below: \n",
    "\n",
    "(1) Raj has a **beautiful red car**\n",
    "\n",
    "(2) Raj has a **charismatic magnetic personality**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qAGB1_S8NThy"
   },
   "outputs": [],
   "source": [
    "testSent1 = \"There was a sudden jerk, a terrific convulsion of the limbs; and there he\"\n",
    "testSent2 = \"They made room for the stranger, but he sat down\"\n",
    "testSent3 = \"The hungry and destitute situation of the infant orphan was duly reported by\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "yLY1ymH-ZuJu"
   },
   "outputs": [],
   "source": [
    "# Generating Models for Bigram and Trigram\n",
    "\n",
    "\n",
    "V=len(unigrams)\n",
    "\n",
    "\n",
    "# The bigram model is a dictionary where the key (word1, word2) stores the probability of word2 occurring, given word1 has occurred.\n",
    "bigrams_model={}\n",
    "for tup1 in unigrams_freqDist:\n",
    "    for tup2 in unigrams_freqDist:\n",
    "        bigrams_model[(tup1[0],tup2[0])]=(bigrams_freqDist.get((tup1[0], tup2[0]), 0) + 1)/(unigrams_freqDist[tup1] + V)\n",
    "        #tup2[0], given tup1[0]\n",
    "        \n",
    "# The trigram model is a dictionary where the key (word1, word2, word3) stores the probability of word3 occurring, given (word1, word2) has occurred.\n",
    "trigrams_model={}\n",
    "for (x1,x2) in bigrams_freqDist:\n",
    "    for x3 in unigrams_freqDist:\n",
    "        trigrams_model[(x1,x2,x3[0])] = (trigrams_freqDist.get((x1,x2,x3[0]), 0) + 1)/(bigrams_freqDist[(x1,x2)] + V)\n",
    "        #x3[0], given (x1,x2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating functions to predict next word for a given sequence\n",
    "\n",
    "# Prediction using bigram\n",
    "# Given w1, predict w2\n",
    "def pred_bigram(w1):\n",
    "    max_prob=0\n",
    "    w2=\"\"\n",
    "    for (key,value) in bigrams_model.items():\n",
    "        if(key[0]==w1):\n",
    "            if value>max_prob:\n",
    "                max_prob=value\n",
    "                w2=key[1]\n",
    "    return w2 \n",
    "\n",
    "\n",
    "# Prediction using trigram\n",
    "# Given w1 and w2, predict w3\n",
    "def pred_trigram(w1,w2):\n",
    "    max_prob=0\n",
    "    w3=\"\"\n",
    "    for (key,value) in trigrams_model.items():\n",
    "        if(key[0]==w1 and key[1]==w2):\n",
    "            if value>max_prob:\n",
    "                max_prob=value\n",
    "                w3=key[2]\n",
    "    return w3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence 1: There was a sudden jerk, a terrific convulsion of the limbs; and there he\n",
      "\n",
      "Predicting next 3 words using Bigram Model:\n",
      "There was a sudden jerk , a terrific convulsion of the limbs ; and there he had been ,\n",
      "\n",
      "Predicting next 3 words using Trigram Model:\n",
      "There was a sudden jerk , a terrific convulsion of the limbs ; and there he sat down ,\n",
      "***************************************************************************\n",
      "\n",
      "Sentence 2: They made room for the stranger, but he sat down\n",
      "\n",
      "Predicting next 3 words using Bigram Model:\n",
      "They made room for the stranger , but he sat down the old gentleman\n",
      "\n",
      "Predicting next 3 words using Trigram Model:\n",
      "They made room for the stranger , but he sat down , was a\n",
      "***************************************************************************\n",
      "\n",
      "Sentence 3: The hungry and destitute situation of the infant orphan was duly reported by\n",
      "\n",
      "Predicting next 3 words using Bigram Model:\n",
      "The hungry and destitute situation of the infant orphan was duly reported by the old gentleman\n",
      "\n",
      "Predicting next 3 words using Trigram Model:\n",
      "The hungry and destitute situation of the infant orphan was duly reported by the side of\n"
     ]
    }
   ],
   "source": [
    "# PREDICTIONS\n",
    "\n",
    "\n",
    "# Sentence 1\n",
    "testSent1 = \"There was a sudden jerk, a terrific convulsion of the limbs; and there he\"\n",
    "print(\"\\nSentence 1:\",testSent1)\n",
    "\n",
    "#using bigram\n",
    "print(\"\\nPredicting next 3 words using Bigram Model:\")\n",
    "sent=list(word_tokenize(testSent1))\n",
    "b1=pred_bigram(sent[len(sent)-1])\n",
    "b2=pred_bigram(b1)\n",
    "b3=pred_bigram(b2)\n",
    "sent=sent+[b1,b2,b3]\n",
    "print(\" \".join(sent))\n",
    "\n",
    "#using trigram\n",
    "print(\"\\nPredicting next 3 words using Trigram Model:\")\n",
    "sent=list(word_tokenize(testSent1))\n",
    "b1=pred_trigram(sent[len(sent)-2], sent[len(sent)-1])\n",
    "b2=pred_trigram(sent[len(sent)-1],b1)\n",
    "b3=pred_trigram(b1,b2)\n",
    "sent=sent+[b1,b2,b3]\n",
    "print(\" \".join(sent))\n",
    "print(\"***************************************************************************\")\n",
    "\n",
    "\n",
    "\n",
    "# Sentence 2\n",
    "testSent2 = \"They made room for the stranger, but he sat down\"\n",
    "print(\"\\nSentence 2:\",testSent2)\n",
    "\n",
    "#using bigram\n",
    "print(\"\\nPredicting next 3 words using Bigram Model:\")\n",
    "sent=list(word_tokenize(testSent2))\n",
    "b1=pred_bigram(sent[len(sent)-1])\n",
    "b2=pred_bigram(b1)\n",
    "b3=pred_bigram(b2)\n",
    "sent=sent+[b1,b2,b3]\n",
    "print(\" \".join(sent))\n",
    "\n",
    "#using trigram\n",
    "print(\"\\nPredicting next 3 words using Trigram Model:\")\n",
    "sent=list(word_tokenize(testSent2))\n",
    "b1=pred_trigram(sent[len(sent)-2], sent[len(sent)-1])\n",
    "b2=pred_trigram(sent[len(sent)-1],b1)\n",
    "b3=pred_trigram(b1,b2)\n",
    "sent=sent+[b1,b2,b3]\n",
    "print(\" \".join(sent))\n",
    "print(\"***************************************************************************\")\n",
    "\n",
    "\n",
    "\n",
    "#Sentence 3\n",
    "testSent3 = \"The hungry and destitute situation of the infant orphan was duly reported by\"\n",
    "print(\"\\nSentence 3:\",testSent3)\n",
    "\n",
    "#using bigram\n",
    "print(\"\\nPredicting next 3 words using Bigram Model:\")\n",
    "sent=list(word_tokenize(testSent3))\n",
    "b1=pred_bigram(sent[len(sent)-1])\n",
    "b2=pred_bigram(b1)\n",
    "b3=pred_bigram(b2)\n",
    "sent=sent+[b1,b2,b3]\n",
    "print(\" \".join(sent))\n",
    "\n",
    "#using trigram\n",
    "print(\"\\nPredicting next 3 words using Trigram Model:\")\n",
    "sent=list(word_tokenize(testSent3))\n",
    "b1=pred_trigram(sent[len(sent)-2], sent[len(sent)-1])\n",
    "b2=pred_trigram(sent[len(sent)-1],b1)\n",
    "b3=pred_trigram(b1,b2)\n",
    "sent=sent+[b1,b2,b3]\n",
    "print(\" \".join(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following function gives the perplexity score for a given sentence, for all three models\n",
    "\n",
    "def perplexity(sentence):\n",
    "    \n",
    "    V=len(unigrams)\n",
    "    \n",
    "    words=word_tokenize(sentence)#tokenizing into words\n",
    "\n",
    "    #if the word is not alphanumeric we remove it. We also remove apostrophe and first brackets\n",
    "    for word in words:\n",
    "        if word.isalnum==False or word==\"'\" or word==\"(\" or word==\")\":\n",
    "            words.remove(word)\n",
    "\n",
    "    # we convert all words to lower case        \n",
    "    for i in range(len(words)):\n",
    "        words[i]=words[i].lower()\n",
    "    \n",
    "    test_uni = []\n",
    "    test_bi = []\n",
    "    test_tri = []\n",
    "    \n",
    "    test_uni.extend(ngrams(words,1))\n",
    "    test_bi.extend(ngrams(words,2))\n",
    "    test_tri.extend(ngrams(words,3))\n",
    "    \n",
    "    uni_perplexity = 1\n",
    "    bi_perplexity = 1\n",
    "    tri_perplexity = 1\n",
    "    \n",
    "    for word in test_uni:\n",
    "        uni_perplexity *= V/unigrams_freqDist.get(word, 1)\n",
    "    uni_perplexity**(1/len(test_uni))\n",
    "    \n",
    "    for words in test_bi:\n",
    "        bi_perplexity *= 1/bigrams_model.get(words, 1/V)\n",
    "    bi_perplexity**(1/len(test_bi))\n",
    "    \n",
    "    for words in test_tri:\n",
    "        tri_perplexity *= 1/trigrams_model.get(words, 1/V)\n",
    "    tri_perplexity**(1/len(test_tri))\n",
    "    \n",
    "    print(\"Unigram Model's Perplexity score: \",uni_perplexity)\n",
    "    print(\"Bigram Model's Perplexity score: \",bi_perplexity)\n",
    "    print(\"Trigram Model's Perplexity score: \",tri_perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Sentence 3:\n",
      "Unigram Model's Perplexity score:  3.532002093150471e+38\n",
      "Bigram Model's Perplexity score:  1.1277441573727877e+48\n",
      "Trigram Model's Perplexity score:  2.1484582331905204e+46\n"
     ]
    }
   ],
   "source": [
    "# finding perplexity score for sentence 3\n",
    "\n",
    "print(\"For Sentence 3:\")\n",
    "perplexity(testSent3)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NLP_CS60075_A21_Assn1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
